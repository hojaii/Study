* 빅 데이터 3대 요소
> Volumn(규모)
	# 데이터의 급증은 빅 데이터의 큰 특징
	# 적게는 PB에서 많게는 ZB이상을 기준으로 보지만 정량적으로는 정해져있지 않음
	# 데이터의 양은 산업별, 규모별 차이가 있지만 대량이라는 점에서 일치
> Velocity(속도)
	# 변화의 속도 또는 유통의 속도가 빠른 데이터
	# 주식, 환율, 항공 경로 등 매우 짧은 시간 내에 계속 변경되는 데이터
> Variety(다양성)
	# 소셜 데이터, 위치 데이터, 텍스트, 센서 데이터, 비디오, 오디오 등 다양한 형태의 데이터
	# 어떻게 다양한 데이터를 수집, 저장, 처리, 분석하느냐가 이슈로 등장
	# 형태에 따른 빅데이터 : 정형데이터, 비정형데이터, 고정데이터

* 빅 데이터 개념의 확대 (6V)
> 3V정의에 따른 빅데이터 : 대규모, 고속의 다양한 데이터를 분석하여 인사이트와 가치를 주는 기술
> 이후 IBM은 Verecity(신뢰성)을 추가하고 4V정의를 내림
> 현재는 6V까지 확장된 개념이 널리 통용됨(3V + 2V(Veracity, Visualization) + Value)
> 6V정의에 따른 빅데이터 : 대규모로 빠르게 발생하고있는 다양한 데이터를 수용하고 정확한 분석을 통하여
신뢰성을 확보하고, 시각하하여 새로운 가치를 창출하는 기술

* 빅 데이터를 처리하는 기술(Apache Hadoop)
> 저가 서버와 하드디스크를 이용해서 빅 데이터를 상대적으로 쉽게 활용해 처리할 수 있는 분산 파일시스템
> 빅 데이터 플랫폼의 핵심기술이자 사실상의 표준
	# HDFS(분산파일시스템)과 MapReduce(분산 병렬 처리 기술)로 구성


* Hadoop
> 하둡이란?
	# 대용량 데이터 처리를 위해 개발된 오픈 소스 플랫폼
	# 구성요소
		1. HDFS : 대용량 파일의 분산저장
		2. MapReduce : 분산 저장된 파일을 이용, 데이터를 처리
	# 아래 두가지 문제를 고려하고 안정적이고 확장성이 높은 저장 및 분석 플랫폼을 제공
> 단일 디스크 사용의 문제
	1. 데이터를 읽고 쓰는데 너무 많은 시간이 걸림
	2. 가장 쉬운 해결책은 여러 개의 디스크에 데이터를 동시에 입출력하는 것


* HDFS
> HDFS : 대용량 파일을 여러 서버에 나누어 저장하고 다양한 클라이언트가 빠르게 처리할 수 있도록 설계된 파일 시스템
	# HDFS가 제공하는 다양한 API를 활용하여 파일을 읽고 저장할 수있음
	# 장애 복구, 대용량 파일저장, 데이터 무결성등 지원
> 블록 구조의 파일 시스템
	# 파일은 일정한 크기의 블록으로 분할되어 복수개의 복제본을 분산된 서버에 나누어 저장
> 네임노드
	#HDFS 시스템을 유지하기 위한 메타데이터를 관리하고 유실방지를 위한 모든 변경이력을 저장
	# 데이터 노드에 장애가 발생하면 다른 데이터 노드에 블록을 복제, 항상 같은 수의 복제본을 유지
> 데이터 노드
	# HDFS에 저장되는 파일이 블록으로 나누어져 실제 저장되는 장소

* MapReduce
> 한번의 쿼리로 전체 혹은 상당규모의 데이터셋을 처리
	# 맵 리듀스는 일괄 질의 처리기
	# 전체 데이터셋을 대상으로 비정형 쿼리를 수행
	# 합리적인 시간 내에 결과를 보여주는데 집중
> 맵 리듀스의 강점은 일괄처리 시스템이기 때문에 대화형 분석에는 적합하지 않음

*Hadoop vs RDBMS
> RDBMS : 지속적으로 변경되는 적은 양의 데이터셋에 적함
> MapReduce : 데이터를 한번 쓰고 여러번 읽는 대량의 데이터셋, 어플에 적합함

		RDBMS			MapReduce
데이터크기	기가바이트		페타바이트
접근 방식		대화형과 일괄처리		일괄처리
변경		여러번 읽고 쓰기		한번쓰고 여러번 읽기
트랜잭션		ACID			없음
구조		쓰기 기준 스키마		읽기 기준 스키마
무결성		높음			낮음
확장성		비선형			선형

> 데이터셋 내부에서 처리되는 구조
	> 정형데이터 : XML, 스키마를 가진 데이터베이스 테이블 등
	> 반정형데이터 : 정형데아터에 비해 스키마가 유연하거나 생략된 데이터
	> 비정형데이터 : 스키마 자체가 없는 데이터들
> 중요한 개념
	> 읽기시점스키마 : 하둡은 처리시점에서 데이터를 해석하도록 설계
		#유연성을 제공하고, 데이터를 불러오는 비용이 많이드는 단계를 피할수 있음
	> 데이터 지역성 : 계산 노드와 데이터 노드를 함께 배치하여 성능을 높임
		# 데이터가 있는 곳에 계산 로직을 보냄

* 수집 레이어
> 데이터 수집
	# 빅 데이터 시스템에서 가장 중요
	# 빅 데이터 시스템의 궁극적 목표는 수집된 데이터를 HDFS등의 분산 파일 시스템에 저장하는 것

* 저장 레이어
> 데이터 저장 처리
	# 모인 데이터들을 저장하고 처리하는 역할을 담당함
	# 빅데이터 시스템의 핵심이라 할 수 있는 하둡
	# 하둡의 두가지 구성요소
		1. HDFS
		2. MapReduce

* 데이터 마이닝(분석)
> 대용량 데이터에서 특정 패턴을 찾기위한 과정
> 데이터에서 어떤 의미를 찾아서 가치를 생성하는 과정
> 데이터 사이언스(통계/수학) 기법이 활용된다

* 빅 데이터에 대한 오해와 한계
> 빅데이터의 한계 : 그 데이터가 가지고 있는 데이터 자체의 한계
	# 분석하고자 하는 빅 데이터가 어떤 한계를 가지고있는지를 항상염두에 두고 그 데이터의 속성을 제대로 이해해야함

